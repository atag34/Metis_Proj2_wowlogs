{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warcraft Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I will be using data scraped from [warcraftlogs](https://classic.warcraftlogs.com/) to build a linear regression model that can predict how long it will take a group to kill the WoW Classic raid boss Onyxia.\n",
    "\n",
    "While the website contains information for all raid bosses in the game, I felt it would be best to focus on one for this project due to the very large differences in fight mechanics and relative difficulty. Models should likely be tuned for individual fights for improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to collect a list of links to fight summaries for the specific raid boss we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty dictionary to store links and guild name information.\n",
    "wow_logs = {}\n",
    "\"\"\"\n",
    "Because I wanted to get a larger time span of data, I will skip 40 pages at a time and pull all the reports from those pages\n",
    "This will give us data going back to about January. This should help us capture a larger number of groups, some who may\n",
    "be very familair with the fight by now, and those who may only just be starting the content. ex.(40,80,120...)\n",
    "\"\"\"\n",
    "for i in range(0,2000,40):\n",
    "    url = 'https://classic.warcraftlogs.com/zone/reports?zone=1001&page='+str(i)\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "\n",
    "    # Pull the relevant table of information which contains the link to the report as well as guild information.\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    table=soup.find('table')\n",
    "    rows = [row for row in table.find_all('tr')]\n",
    "    \n",
    "    # Iterate through the rows of the table and add them to the dictionary\n",
    "    for row in rows:\n",
    "        #gets report link\n",
    "        link = row.find_all('td')[0].find('a')['href']\n",
    "    \n",
    "        #name of guild\n",
    "        guild = row.find_all('td')[2].text\n",
    "        \n",
    "        #add to dictionary\n",
    "        wow_logs[link] = [link,guild]\n",
    "\n",
    "\n",
    "len(wow_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 5000 links to run through. It is important to remember for rerunning that, this will now pull different data as new logs are posted every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling the Fight Summary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I will initiate a selenium webdriver as we will have to interact with certain elements of the page and load some javascript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate where in my local machine the geckodriver is for using firefox.\n",
    "driver = webdriver.Firefox(executable_path=r'C:/Users/atag3/Desktop/NYC DSA Python EDA/Untitled Folder/geckodriver.exe')\n",
    "\n",
    "# we have to click through the first message accepting cookies. this should only need to be done once a window is open.\n",
    "driver.get('https://classic.warcraftlogs.com/reports/b2tKdk1xmj7pw43z')\n",
    "butt = driver.find_elements_by_xpath('/html/body/div[2]/div/div/div/div[2]/div/button[3]')\n",
    "butt[0].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A New empty dictionary to store our values.\n",
    "new_logs = {}\n",
    "\n",
    "# A counter in case our scraping hits an error or stops, this will make it easier to resume where we hit an error.\n",
    "counter_place = 0\n",
    "\n",
    "# There was some sort of memory leak issue that cause the firefox driver to crash after some amount of time. This will act as\n",
    "# a counter to close and start a new driver after going through 100 reports.\n",
    "subcounter = 0\n",
    "\n",
    "#now we want to go through each link and pull the relevant info.\n",
    "for links in list(wow_logs.keys())[counter_place:len(wow_logs.keys())]:\n",
    "    \n",
    "    # Navigate to the fight summary page\n",
    "    link = 'https://classic.warcraftlogs.com'+links\n",
    "    driver.get(link)\n",
    "    \n",
    "    # Look for boss fights recorded in the log and return the 3rd boss fight. It should always be onyxia.\n",
    "    p_element = driver.find_elements_by_class_name(\"report-overview-boss-caption\")\n",
    "    \n",
    "    # If the page correctly loaded and the fight summary is there move on and collect the data. Otherwise skip to the next.\n",
    "    if len(p_element)==0:\n",
    "        pass\n",
    "    else:\n",
    "    \n",
    "        try:\n",
    "            #click the onyxia fight to give us the fight report\n",
    "            p_element[2].click()\n",
    "            time.sleep(.5+2*random.random())\n",
    "\n",
    "            #at this point we have made it to the fight summary page we want.\n",
    "\n",
    "            # First I want to make sure we are looking at Onyxia\n",
    "            boss_icon = driver.find_elements_by_id(\"filter-fight-boss-icon\")\n",
    "            is_ony = 'Onyxia' if re.search('1084-icon.jpg',boss_icon[0].get_attribute('src')) else 'Not Onyxia'\n",
    "\n",
    "            #collect the fight time and status\n",
    "            kill_wipe= driver.find_elements_by_id(\"filter-fight-details-text\")\n",
    "\n",
    "            #is this a wipe or kill?\n",
    "            is_kill = kill_wipe[0].text.split(\" (\")[0]\n",
    "\n",
    "            #how long did it take? format in MM:SS\n",
    "            time_f = kill_wipe[0].text.split('(')[1].split(')')[0]\n",
    "\n",
    "            # look at character details. The format is Class (Specialization), so we can use that to identify how many of each\n",
    "            # class show up in the list.\n",
    "            player_info = driver.find_elements_by_class_name(\"character-details-contents\")\n",
    "            chars = player_info[0].text.split('\\n')\n",
    "            \n",
    "            #if a line matches our Class (Specialization) format, add it to a list\n",
    "            classes = [i for i in chars if '(' in i] \n",
    "            \n",
    "            # make a dict of class (spec) counts\n",
    "            classes = Counter(classes)\n",
    "\n",
    "            # Average Item Level. The list of text indicates tanks then healers then dps. We can use these words to seperate the \n",
    "            # groups of players into their roles, and take an average of their item level.\n",
    "            tanks_end=chars.index('DPS')\n",
    "            dps_end=chars.index('Healers')\n",
    "            heals_end=len(chars)\n",
    "\n",
    "            #Average Ilvl of tanks. Elements in the list that contain a numeric digit are always item level information.\n",
    "            ilvls = [re.findall(\"\\d+\", i) for i in chars[0:tanks_end] if 'Item Level' in i]\n",
    "            flattened_ilvl = [int(val) for sublist in ilvls for val in sublist]\n",
    "            tank_ilvls = np.mean(flattened_ilvl)\n",
    "\n",
    "            #Average Ilvl of DPS\n",
    "            ilvls = [re.findall(\"\\d+\", i) for i in chars[tanks_end:dps_end] if 'Item Level' in i]\n",
    "            flattened_ilvl = [int(val) for sublist in ilvls for val in sublist]\n",
    "            dps_ilvls = np.mean(flattened_ilvl)\n",
    "\n",
    "            #Average Ilvl of Healers\n",
    "            ilvls = [re.findall(\"\\d+\", i) for i in chars[dps_end:heals_end] if 'Item Level' in i]\n",
    "            flattened_ilvl = [int(val) for sublist in ilvls for val in sublist]\n",
    "            heals_ilvls = np.mean(flattened_ilvl)\n",
    "\n",
    "            # World Buff Info. This returns how many world buffs are on the entire group.\n",
    "            buff_info = driver.find_elements_by_xpath('/html/body/div[3]/div[2]/div[6]/div[3]/div[1]/div[7]/div[3]/div[2]/div[7]/div[2]/div[*]/div/table/tbody/tr[*]/td[3]/a[*]/img')\n",
    "            w_buff = len(buff_info)\n",
    "\n",
    "            # Add the information we have scrapped to the dictionary.\n",
    "            new_logs[links] = {'boss':is_ony,\n",
    "                               'kill':is_kill,\n",
    "                               'fight_time':time_f,\n",
    "                               'tank_avg_ilvl': tank_ilvls,\n",
    "                               'dps_avg_ilvl': dps_ilvls,\n",
    "                               'heals_avg_ilvl': heals_ilvls,\n",
    "                               'world_buffs': w_buff\n",
    "                              }\n",
    "            \n",
    "            # add the counts of class (spec) dictionary to our dictionary.\n",
    "            new_logs[links].update(dict(zip(list(classes.keys()),list(classes.values()))))\n",
    "\n",
    "            counter_place += 1\n",
    "            subcounter += 1\n",
    "            \n",
    "            # For every 100 reports we go through, we will close the window, open a new one and click the cookie agreement.\n",
    "            if subcounter >=100:\n",
    "                driver.quit()\n",
    "                # Start up remote window\n",
    "                driver = webdriver.Firefox(executable_path=r'C:/Users/atag3/Desktop/NYC DSA Python EDA/Untitled Folder/geckodriver.exe')\n",
    "                driver.get('https://classic.warcraftlogs.com/reports/b2tKdk1xmj7pw43z')\n",
    "\n",
    "                # we have to click through the first message accepting cookies. this should only nee to be done once.\n",
    "                butt = driver.find_elements_by_xpath('/html/body/div[2]/div/div/div/div[2]/div/button[3]')\n",
    "                butt[0].click()\n",
    "                print(\"reset browser\")\n",
    "                \n",
    "                #reset the subcounter\n",
    "                subcounter=0\n",
    "                \n",
    "        except Exception:\n",
    "            # A generic error printout. \n",
    "            # We have enough data to iterate through that losing some reports from errors should be fine.\n",
    "            \n",
    "            print(\"oh no an exception\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have our data in a dictionary, and we can simply convert it to a pandas dataframe and pickle it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(new_logs, orient='index')\n",
    "df.to_pickle('wowlogs_data_v2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
